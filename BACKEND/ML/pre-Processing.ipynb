{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import tika\n",
    "from tika import parser # pip install tika"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visulizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d -p 9998:9998 logicalspark/docker-tikaserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Resume\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "Contact\n",
      "\n",
      "www.linkedin.com/in/aju-\n",
      "palleri-248798a4 (LinkedIn)\n",
      "\n",
      "Top Skills\n",
      "Teaching\n",
      "Core Java\n",
      "Python (Programming Language)\n",
      "\n",
      "Certifications\n",
      "Use WordPress to Create a Blog for\n",
      "your Business\n",
      "Create Your First Web App with\n",
      "Python and Flask\n",
      "Excel Skills for Business: Essentials\n",
      "Introduction to Relational Database\n",
      "and SQL\n",
      "Build a Full Website using\n",
      "WordPress\n",
      "\n",
      "Aju Palleri\n",
      "Assistant Professor at PCE, New Panvel\n",
      "Mumbai\n",
      "\n",
      "Summary\n",
      "I am working as an Assistant Professor with PCE, New Panvel for\n",
      "past 10 years. I have keen interest in programming and learning\n",
      "new languages. C, C++, Java, Python, DBMS, Digital Logic Design,\n",
      "Computer Simulation and modelling  and web programming are\n",
      "subjects taught to students.\n",
      "\n",
      "I have mentored students to National Level Smart India Hackathon\n",
      "and Deep Blue Competition. I have guided SE, TE and BE students\n",
      "in their projects and it has been fun to bring in creative ideas and\n",
      "develop using different technologies I am part of internal software\n",
      "development team, responsible to develop software that maintains\n",
      "academic records of students.\n",
      "\n",
      "Experience\n",
      "\n",
      "Pillai College of Engineering\n",
      "Assistant Professor at PCE, New Panvel\n",
      "August 2011 - Present (10 years 6 months)\n",
      "Navi Mumbai\n",
      "\n",
      "AC Nielsen\n",
      "DA Junior Programmer\n",
      "January 2011 - July 2011 (7 months)\n",
      "Byculla, Mumbai\n",
      "\n",
      "Creating surveys using Java Script and confirmit tool.\n",
      "\n",
      "Education\n",
      "Pillai College of Engineering\n",
      "Master's degree, Information Technology · (July 2012 - August 2015)\n",
      "\n",
      "Bharati Vidyapeeth's College of Engineering\n",
      "Bachelor in Engineering, Information Technology · (July 2004 - May 2008)\n",
      "\n",
      "  Page 1 of 2\n",
      "\n",
      "https://www.linkedin.com/in/aju-palleri-248798a4?jobid=1234&lipi=urn%3Ali%3Apage%3Ad_jobs_easyapply_pdfgenresume%3BWezi4k4uTPKgYQqnqv7u9g%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_jobs_easyapply_pdfgenresume-v02_profile\n",
      "https://www.linkedin.com/in/aju-palleri-248798a4?jobid=1234&lipi=urn%3Ali%3Apage%3Ad_jobs_easyapply_pdfgenresume%3BWezi4k4uTPKgYQqnqv7u9g%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_jobs_easyapply_pdfgenresume-v02_profile\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "  Page 2 of 2\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Method 1 \n",
    "#os.environ['TIKA_SERVER_JAR'] = 'https://repo1.maven.org/maven2/org/apache/tika/tika-server/1.19/tika-server-1.19.jar'\n",
    "#tika.initVM()\n",
    "raw = parser.from_file(\"/Users/cosmos/Desktop/Deepblue/DeepBlue/Dataset-Scraping/Resumes/linkdien.pdf\")\n",
    "print(raw['content'])\n",
    "text = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2 \n",
    "import textract\n",
    "text = textract.process(\"/Users/cosmos/Desktop/Deepblue/DeepBlue/Dataset-Scraping/Resumes/linkdien.pdf\")\n",
    "#text = textract.process('/Users/cosmos/Desktop/Deepblue/DeepBlue/Dataset-Scraping/Resumes/Word Doc Resume #6.docx')\n",
    "text = str(text)\n",
    "text=re.sub(\"\\n\", \" \",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre - Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.linkedin.com/in/aju-palleri-248798a4?jobid=1234&lipi=urn%3Ali%3Apage%3Ad_jobs_easyapply_pdfgenresume%3BWezi4k4uTPKgYQqnqv7u9g%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_jobs_easyapply_pdfgenresume-v02_profile'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#links\n",
    "url = re.search(\"(?P<url>https?://[^\\s]+)\", text).group(\"url\")\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contact@tutorialspoint.com', 'feedback@tp.com']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting Email & numbers\n",
    "\n",
    "extracted_text={}\n",
    "\n",
    "\n",
    "emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", text)\n",
    "\n",
    "def get_phone_numbers(string):\n",
    "    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "    phone_numbers = r.findall(string)\n",
    "    return [re.sub(r'\\D', '', num) for num in phone_numbers]\n",
    "\n",
    "#finding them\n",
    "phone_number= get_phone_numbers(text)\n",
    "\n",
    "#extracted_text['E-mail'] = email\n",
    "extracted_text['Phone number'] = phone_number\n",
    "\n",
    "phone_number\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi i am obama davis .Please contact us at contact@tutorialspoint.com for further information. name is Om Surve from India.You can also give feedback at feedback@tp.com, My number is 9869249403, page 1 of 1, date : 10-1-2022, 10/10/2121\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi i am obama davis .please contact us at contact for further information. name is om surve from india.you can also give feedback at feedback my number is , , date : 10-1-2022, 10/10/2121'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_stop = [\"page 1 of 1\",\"Resume\", \"page 1 of 2\",\"page 1 of 3\", \"page 1 of 4\", \n",
    "                \"page 2 of 2\",\"page 3 of 3\",\"page 4 of 4\",\"page 2 of 3\",\n",
    "                \"page 2 of 4\",\"page 3 of 4\",\"resume\"]\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = list(stop_words.union(words_stop))\n",
    "\n",
    "def pre_process(text):\n",
    "    text = \"\".join(text.split('\\n')) #remove whitespaces\n",
    "    text = text.lower()\n",
    "    \n",
    "    #using re\n",
    "    text=re.sub('http\\S+\\s*',' ',text)\n",
    "    text=re.sub('RT|cc',' ',text)\n",
    "    text=re.sub('#\\S+',' ',text)\n",
    "    text=re.sub('@\\S+',' ',text)\n",
    "    \n",
    "    for i in range (len(emails)): #removes emails\n",
    "        text = text.replace(emails[i],\"\") \n",
    "    \n",
    "    text = re.sub(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})','',text) #removes phone numbers\n",
    "    text=re.sub(r'[^\\x00-\\x7f]',' ',text)\n",
    "    text=re.sub('\\s+',' ',text)\n",
    "    text=re.sub(\"\\n\", \" \",text)\n",
    "    \n",
    "    #remove uncessary stop words\n",
    "    for i in range (len(words_stop)): #removes emails\n",
    "        text = text.replace(words_stop[i],\"\") \n",
    "    \n",
    "    return ''.join(text)\n",
    "\n",
    "\n",
    "pre_process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numbers\n",
    "\n",
    "#re.sub(r'\\d+','',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the first two maketrans arguments must have equal length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_70532/1730078248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_70532/1730078248.py\u001b[0m in \u001b[0;36mrp\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtranslator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: the first two maketrans arguments must have equal length"
     ]
    }
   ],
   "source": [
    "#remove punctaions\n",
    "\n",
    "def rp(text):\n",
    "    translator=str.maketrans(\",\",string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "rp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing deauflt stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words= set(stopwords.words(\"english\"))\n",
    "    word_tokens=word_tokenize(text)\n",
    "    new_text= [word for word in word_tokens if word not in stopwords]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (224491941.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_32176/224491941.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    stems = stemmer1.stem(word) for word in word_tokens\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#stemming is it necsaary?\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer1 = SnowballStemmer(language='english')\n",
    "\n",
    "def stemmer(text):\n",
    "    word_tokens=word_tokenize(text)\n",
    "    stems = [stemmer1.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "stemmer(text)\n",
    "#stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'Entry\",\n",
       " 'Level',\n",
       " 'Resume',\n",
       " '(',\n",
       " 'Delete',\n",
       " 'this',\n",
       " 'tag',\n",
       " ')',\n",
       " '\\\\n\\\\n\\\\n\\\\nAlex',\n",
       " 'Carter\\\\n\\\\nPosition',\n",
       " 'Title\\\\n\\\\n\\\\n\\\\nPhone',\n",
       " 'Number\\\\n\\\\n\\\\n\\\\nLocation\\\\n\\\\n\\\\n\\\\nname',\n",
       " '@',\n",
       " 'email.com\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nEDUCATION\\\\n\\\\n\\\\n\\\\nDegree',\n",
       " 'Title',\n",
       " ',',\n",
       " 'University\\\\n\\\\nGRADUATION',\n",
       " 'YEAR',\n",
       " ',',\n",
       " 'GPA\\\\n\\\\nLatin',\n",
       " 'Honors',\n",
       " 'Earned',\n",
       " '\\\\nInclude',\n",
       " 'Scholarship',\n",
       " '\\\\nDid',\n",
       " 'you',\n",
       " 'study',\n",
       " 'abroad',\n",
       " '?',\n",
       " '\\\\nInclude',\n",
       " 'that',\n",
       " 'here',\n",
       " '..',\n",
       " '\\\\n\\\\n\\\\n\\\\nTECHNICAL',\n",
       " 'SKILLS\\\\n\\\\n\\\\n\\\\nThis',\n",
       " 'be',\n",
       " 'where',\n",
       " 'you',\n",
       " 'will',\n",
       " 'list',\n",
       " 'all',\n",
       " 'of',\n",
       " '\\\\nthe',\n",
       " 'program',\n",
       " 'and',\n",
       " 'software',\n",
       " 'that',\n",
       " 'you',\n",
       " 'be',\n",
       " 'familiar',\n",
       " 'with',\n",
       " 'using.\\\\n\\\\n\\\\n\\\\nList',\n",
       " 'Item',\n",
       " '1\\\\n\\\\nList',\n",
       " 'Item',\n",
       " '2\\\\n\\\\nList',\n",
       " 'Item',\n",
       " '3\\\\n\\\\nList',\n",
       " 'Item',\n",
       " '4\\\\n\\\\nList',\n",
       " 'Item',\n",
       " '5\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSUMMARY\\\\n\\\\n\\\\n\\\\nUse',\n",
       " 'this',\n",
       " 'space',\n",
       " 'to',\n",
       " 'write',\n",
       " 'a',\n",
       " 'two',\n",
       " 'or',\n",
       " 'three',\n",
       " 'sentence',\n",
       " 'summary',\n",
       " 'of',\n",
       " 'who',\n",
       " 'you',\n",
       " 'be',\n",
       " 'as',\n",
       " 'a',\n",
       " 'professional',\n",
       " '.',\n",
       " 'Include',\n",
       " 'that',\n",
       " 'you',\n",
       " 'be',\n",
       " 'an',\n",
       " 'entry',\n",
       " 'level',\n",
       " 'employee',\n",
       " ',',\n",
       " 'and',\n",
       " 'because',\n",
       " 'you',\n",
       " 'be',\n",
       " 'entry',\n",
       " 'level',\n",
       " 'take',\n",
       " 'this',\n",
       " 'as',\n",
       " 'an',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'mention',\n",
       " 'who',\n",
       " 'you',\n",
       " 'be',\n",
       " 'as',\n",
       " 'an',\n",
       " 'employee',\n",
       " '.',\n",
       " 'Include',\n",
       " 'whatever',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'your',\n",
       " 'industry',\n",
       " 'that',\n",
       " 'you',\n",
       " 'do',\n",
       " 'have',\n",
       " '.',\n",
       " '\\\\n\\\\nSee',\n",
       " 'my',\n",
       " 'personal',\n",
       " 'website',\n",
       " 'and',\n",
       " 'blog',\n",
       " 'at',\n",
       " 'www.YourWebsiteHere.com\\\\n\\\\n\\\\n\\\\nEXPERIENCE\\\\n\\\\n\\\\nYour',\n",
       " 'Job',\n",
       " 'Title',\n",
       " 'Here',\n",
       " '2020',\n",
       " '-',\n",
       " 'PRESENT',\n",
       " '\\\\n\\\\nCompany',\n",
       " 'Name',\n",
       " ',',\n",
       " 'Location\\\\n\\\\nWhether',\n",
       " 'this',\n",
       " 'position',\n",
       " 'be',\n",
       " 'an',\n",
       " 'internship',\n",
       " ',',\n",
       " 'part-time',\n",
       " ',',\n",
       " 'or',\n",
       " 'full-time',\n",
       " 'position',\n",
       " ',',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'treat',\n",
       " 'the',\n",
       " 'same',\n",
       " '.',\n",
       " 'This',\n",
       " 'be',\n",
       " 'where',\n",
       " 'you',\n",
       " 'write',\n",
       " 'your',\n",
       " 'responsibilities',\n",
       " 'in',\n",
       " 'this',\n",
       " 'position',\n",
       " 'and',\n",
       " 'how',\n",
       " 'your',\n",
       " 'action',\n",
       " 'in',\n",
       " 'your',\n",
       " 'role',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'overall',\n",
       " 'success',\n",
       " 'for',\n",
       " 'the',\n",
       " 'company.\\\\n\\\\n\\\\n\\\\nYour',\n",
       " 'Job',\n",
       " 'Title',\n",
       " 'Here',\n",
       " '2020',\n",
       " '-',\n",
       " 'PRESENT',\n",
       " '\\\\n\\\\nCompany',\n",
       " 'Name',\n",
       " ',',\n",
       " 'Location\\\\n\\\\nWhether',\n",
       " 'this',\n",
       " 'position',\n",
       " 'be',\n",
       " 'an',\n",
       " 'internship',\n",
       " ',',\n",
       " 'part-time',\n",
       " ',',\n",
       " 'or',\n",
       " 'full-time',\n",
       " 'position',\n",
       " ',',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'treat',\n",
       " 'the',\n",
       " 'same',\n",
       " '.',\n",
       " 'This',\n",
       " 'be',\n",
       " 'where',\n",
       " 'you',\n",
       " 'write',\n",
       " 'your',\n",
       " 'responsibilities',\n",
       " 'in',\n",
       " 'this',\n",
       " 'position',\n",
       " 'and',\n",
       " 'how',\n",
       " 'your',\n",
       " 'action',\n",
       " 'in',\n",
       " 'your',\n",
       " 'role',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'overall',\n",
       " 'success',\n",
       " 'for',\n",
       " 'the',\n",
       " 'company.\\\\n\\\\n\\\\n\\\\nYour',\n",
       " 'Job',\n",
       " 'Title',\n",
       " 'Here',\n",
       " '2020',\n",
       " '-',\n",
       " 'PRESENT',\n",
       " '\\\\n\\\\nCompany',\n",
       " 'Name',\n",
       " ',',\n",
       " 'Location\\\\n\\\\nWhether',\n",
       " 'this',\n",
       " 'position',\n",
       " 'be',\n",
       " 'an',\n",
       " 'internship',\n",
       " ',',\n",
       " 'part-time',\n",
       " ',',\n",
       " 'or',\n",
       " 'full-time',\n",
       " 'position',\n",
       " ',',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'treat',\n",
       " 'the',\n",
       " 'same',\n",
       " '.',\n",
       " 'This',\n",
       " 'be',\n",
       " 'where',\n",
       " 'you',\n",
       " 'write',\n",
       " 'your',\n",
       " 'responsibilities',\n",
       " 'in',\n",
       " 'this',\n",
       " 'position',\n",
       " 'and',\n",
       " 'how',\n",
       " 'your',\n",
       " 'action',\n",
       " 'in',\n",
       " 'your',\n",
       " 'role',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'overall',\n",
       " 'success',\n",
       " 'for',\n",
       " 'the',\n",
       " 'company.\\\\n\\\\n\\\\n\\\\nFREELANCE',\n",
       " 'WORK\\\\n\\\\nWhether',\n",
       " 'this',\n",
       " 'position',\n",
       " 'be',\n",
       " 'an',\n",
       " 'internship',\n",
       " ',',\n",
       " 'part-time',\n",
       " ',',\n",
       " 'or',\n",
       " 'full-time',\n",
       " 'position',\n",
       " ',',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'treat',\n",
       " 'the',\n",
       " 'same',\n",
       " '.',\n",
       " 'This',\n",
       " 'be',\n",
       " 'where',\n",
       " 'you',\n",
       " 'write',\n",
       " 'your',\n",
       " 'responsibilities',\n",
       " 'in',\n",
       " 'this',\n",
       " 'position',\n",
       " 'and',\n",
       " 'how',\n",
       " 'your',\n",
       " 'action',\n",
       " 'in',\n",
       " 'your',\n",
       " 'role',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'overall',\n",
       " 'success',\n",
       " 'for',\n",
       " 'the',\n",
       " 'company.\\\\n\\\\n\\\\nInclude',\n",
       " 'the',\n",
       " 'base-line',\n",
       " 'detail',\n",
       " 'of',\n",
       " 'your',\n",
       " 'freelance',\n",
       " 'work',\n",
       " 'here',\n",
       " '.',\n",
       " 'Include',\n",
       " 'the',\n",
       " 'industry',\n",
       " 'you',\n",
       " 'work',\n",
       " 'in',\n",
       " ',',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'your',\n",
       " 'clients',\n",
       " ',',\n",
       " 'and',\n",
       " 'prove',\n",
       " 'success',\n",
       " 'you',\n",
       " 'generate',\n",
       " 'for',\n",
       " 'them',\n",
       " '.',\n",
       " \"'\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer1 = WordNetLemmatizer()\n",
    "\n",
    "def lemtizer(text):\n",
    "    word_tokens=word_tokenize(text)\n",
    "    \n",
    "    text = [lemmatizer1.lemmatize(word,pos= 'v') for word in word_tokens]\n",
    "    return text\n",
    "\n",
    "lemtizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hex codes\n",
    "text = re.sub(r'[^\\x00-\\x7f]',r'', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2022, 10, 1, 0, 0, tzinfo=<UTC>), datetime.datetime(2121, 10, 10, 0, 0, tzinfo=<UTC>)]\n"
     ]
    }
   ],
   "source": [
    "#Extracting data and time\n",
    "\n",
    "from date_extractor import extract_dates\n",
    "\n",
    "extracted_dates = {}\n",
    "\n",
    "dates = extract_dates(text)\n",
    "print(dates)\n",
    "extracted_dates['DATE'] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Om Surve']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "person_list = []\n",
    "person_names=person_list\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "        \n",
    "        \n",
    "names = get_human_names(text)\n",
    "for person in person_list:\n",
    "    person_split = person.split(\" \")\n",
    "    for name in person_split:\n",
    "        if wordnet.synsets(name):\n",
    "            if(name in person):\n",
    "                person_names.remove(person)\n",
    "                break\n",
    "\n",
    "person_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mordecai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_70532/21242484.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#find places\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmordecai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeoparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgeo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeoparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mordecai'"
     ]
    }
   ],
   "source": [
    "#find places\n",
    "\n",
    "from mordecai import Geoparser\n",
    "geo = Geoparser()\n",
    "geo.geoparse(text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
