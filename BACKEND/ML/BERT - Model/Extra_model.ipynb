{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert-large-NER\n",
    "\n",
    "dslim/bert-large-NER - sagemaker\n",
    "\n",
    "* see - examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# gets role for executing training job\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName='{IAM_ROLE_WITH_SAGEMAKER_PERMISSIONS}')['Role']['Arn']\n",
    "hyperparameters = {\n",
    "\t'model_name_or_path':'dslim/bert-large-NER',\n",
    "\t'output_dir':'/opt/ml/model'\n",
    "\t# add your remaining hyperparameters\n",
    "\t# more info here https://github.com/huggingface/transformers/tree/v4.6.1/examples/pytorch/token-classification\n",
    "}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "\tentry_point='run_ner.py',\n",
    "\tsource_dir='./examples/pytorch/token-classification',\n",
    "\tinstance_type='ml.p3.2xlarge',\n",
    "\tinstance_count=1,\n",
    "\trole=role,\n",
    "\tgit_config=git_config,\n",
    "\ttransformers_version='4.6.1',\n",
    "\tpytorch_version='1.7.1',\n",
    "\tpy_version='py36',\n",
    "\thyperparameters = hyperparameters\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dslim/bert-base-NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usage - Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flair/ner-english-large\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/ner-english-large\")\n",
    "\n",
    "# make example sentence\n",
    "sentence = Sentence(\"George Washington went to Washington\")\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence\n",
    "print(sentence)\n",
    "\n",
    "# print predicted NER spans\n",
    "print('The following NER tags are found:')\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tranning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. get the corpus\n",
    "from flair.datasets import CONLL_03\n",
    "\n",
    "corpus = CONLL_03()\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "\n",
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "\n",
    "embeddings = TransformerWordEmbeddings(\n",
    "    model='xlm-roberta-large',\n",
    "    layers=\"-1\",\n",
    "    subtoken_pooling=\"first\",\n",
    "    fine_tune=True,\n",
    "    use_context=True,\n",
    ")\n",
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger = SequenceTagger(\n",
    "    hidden_size=256,\n",
    "    embeddings=embeddings,\n",
    "    tag_dictionary=tag_dictionary,\n",
    "    tag_type='ner',\n",
    "    use_crf=False,\n",
    "    use_rnn=False,\n",
    "    reproject_embeddings=False,\n",
    ")\n",
    "\n",
    "# 6. initialize trainer with AdamW optimizer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.AdamW)\n",
    "\n",
    "# 7. run training with XLM parameters (20 epochs, small LR)\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "trainer.train('resources/taggers/ner-english-large',\n",
    "              learning_rate=5.0e-6,\n",
    "              mini_batch_size=4,\n",
    "              mini_batch_chunk_size=1,\n",
    "              max_epochs=20,\n",
    "              scheduler=OneCycleLR,\n",
    "              embeddings_storage_mode='none',\n",
    "              weight_decay=0.,\n",
    "              )\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
