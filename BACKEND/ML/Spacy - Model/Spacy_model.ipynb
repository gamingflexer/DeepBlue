{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ec9d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date= pickle.load(open('path of the file','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading spacy trained-blank model - en - english\n",
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the traning model\n",
    "\n",
    "def train_model(train_data):\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner=nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner,last=True)\n",
    "        \n",
    "    for _, annotation in train_data:\n",
    "        for ent in annotation['entities']:\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "# Now to remove other pipelines - we define this here - READ SPACY WEBSITE\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(10):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            index = 0\n",
    "            for text, annotations in train_data:\n",
    "                try:\n",
    "                    nlp.update(\n",
    "                        [text],  # batch of texts\n",
    "                        [annotations],  # batch of annotations\n",
    "                        drop=0.2,  # dropout - make it harder to memorise data\n",
    "                        sgd=optimizer,  # callable to update weights\n",
    "                        losses=losses)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                \n",
    "            print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing the trained data & TRAINNING DATA\n",
    "\n",
    "train_model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ee882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "\n",
    "nlp.to_disk('nlp_model_V0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load this already trained model - we use this\n",
    "\n",
    "nlp_model = spacy.load('model name + path')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your testing data\n",
    "\n",
    "test_data= pickle.load(open('path of the file','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f969077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have data in similarly segreagetd form we can use this otherwise not\n",
    "\n",
    "doc = nlp_model(test_data[0][0])\n",
    "for ent in docs.ent:\n",
    "    print (f'{ent. label_. upper (): {30}}- {ent. text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad73e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be4a5cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## To Convert pdf into the data form required we use this\\n\\n\\n-docx extenstion\\n-Images\\n-Other Extension\\n\\n# !pip install PyMuPDF\\n#triedIPyPDF2 but does not work properly.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''## To Convert pdf into the data form required we use this\n",
    "\n",
    "\n",
    "-docx extenstion\n",
    "-Images\n",
    "-Other Extension\n",
    "\n",
    "# !pip install PyMuPDF\n",
    "#triedIPyPDF2 but does not work properly.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee0532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580fa8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, fitz\n",
    "fname = 'pdf path'\n",
    "doc = fitz.open(fname)\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text = text + str(page.getText())\n",
    "\n",
    "# removing spacing between lines    \n",
    "tx = \"\".join(text.split('\\n'))\n",
    "print (text)\n",
    "\n",
    "#and then add the test data oce again \n",
    "\n",
    "doc = nlp_model(text) # add the data varaible in the brackets\n",
    "for ent in docs.ent:\n",
    "    print (f'{ent. label_. upper (): {30}}- {ent. text}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python399jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
