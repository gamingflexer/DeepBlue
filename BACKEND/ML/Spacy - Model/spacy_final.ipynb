{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#train_data= pickle.load(open('train_data_700.pkl','rb')) #300 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'annotation', 'extras', 'metadata'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd #701 rows\n",
    "data = pd.read_json(\"/Users/cosmos/Desktop/Deepblue/ref - kaggle/pre_trained.json\", lines = True)\n",
    "#data = pd.read_json(\"/Users/cosmos/Desktop/Deepblue/DeepBlue/BACKEND/ML/Datasets (UNCLEANED)/NER_RESUME.json\", lines = True)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': ['Email Address'],\n",
       "  'points': [{'start': 1155,\n",
       "    'end': 1198,\n",
       "    'text': 'indeed.com/r/Afreen-Jamadar/8baf379b705e37c6'}]},\n",
       " {'label': ['Links'],\n",
       "  'points': [{'start': 1143,\n",
       "    'end': 1239,\n",
       "    'text': 'https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN'}]},\n",
       " {'label': ['Skills'],\n",
       "  'points': [{'start': 743,\n",
       "    'end': 1140,\n",
       "    'text': 'Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.'}]},\n",
       " {'label': ['Graduation Year'],\n",
       "  'points': [{'start': 729, 'end': 732, 'text': '2016'}]},\n",
       " {'label': ['College Name'],\n",
       "  'points': [{'start': 675,\n",
       "    'end': 702,\n",
       "    'text': 'Shivaji University Kolhapur '}]},\n",
       " {'label': ['Degree'],\n",
       "  'points': [{'start': 631,\n",
       "    'end': 672,\n",
       "    'text': 'Bachelor of Engg in Information Technology'}]},\n",
       " {'label': ['Graduation Year'],\n",
       "  'points': [{'start': 625, 'end': 629, 'text': '2017\\n'}]},\n",
       " {'label': ['College Name'],\n",
       "  'points': [{'start': 614, 'end': 622, 'text': 'CDAC ACTS'}]},\n",
       " {'label': ['Degree'],\n",
       "  'points': [{'start': 606, 'end': 611, 'text': 'PG-DAC'}]},\n",
       " {'label': ['Companies worked at'],\n",
       "  'points': [{'start': 438, 'end': 453, 'text': 'Cisco Networking'}]},\n",
       " {'label': ['Email Address'],\n",
       "  'points': [{'start': 104,\n",
       "    'end': 147,\n",
       "    'text': 'indeed.com/r/Afreen-Jamadar/8baf379b705e37c6'}]},\n",
       " {'label': ['Location'],\n",
       "  'points': [{'start': 62, 'end': 67, 'text': 'Sangli'}]},\n",
       " {'label': ['Name'],\n",
       "  'points': [{'start': 0, 'end': 13, 'text': 'Afreen Jamadar'}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extra has no unique values we can drop it\n",
    "#metadata can be dropped\n",
    "data['annotation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Afreen Jamadar Active member of IIIT Committee...\n",
       "1    Alok Khandai Operational Analyst (SQL DBA) Eng...\n",
       "2    Anvitha Rao Automation developer  - Email me o...\n",
       "3    arjun ks Senior Program coordinator - oracle I...\n",
       "4    Arun Elumalai QA Tester  Chennai, Tamil Nadu -...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"content\"] = data[\"content\"].apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "data[\"content\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "import json\n",
    "import re\n",
    "\n",
    "# JSON formatting functions\n",
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    training_data = []\n",
    "    lines=[]\n",
    "    with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        text = data['content'].replace(\"\\n\", \" \")\n",
    "        entities = []\n",
    "        data_annotations = data['annotation']\n",
    "        if data_annotations is not None:\n",
    "            for annotation in data_annotations:\n",
    "                #only a single point in text annotation.\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                # handle both list of labels or a single label.\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    point_start = point['start']\n",
    "                    point_end = point['end']\n",
    "                    point_text = point['text']\n",
    "\n",
    "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                    if lstrip_diff != 0:\n",
    "                        point_start = point_start + lstrip_diff\n",
    "                    if rstrip_diff != 0:\n",
    "                        point_end = point_end - rstrip_diff\n",
    "                    entities.append((point_start, point_end + 1 , label))\n",
    "        training_data.append((text, {\"entities\" : entities}))\n",
    "    return training_data\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1527844872000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1527845028000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anvitha Rao Automation developer  - Email me o...</td>\n",
       "      <td>[{'label': ['Links'], 'points': [{'start': 288...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1527744637000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arjun ks Senior Program coordinator - oracle I...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 50...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1527834843000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arun Elumalai QA Tester  Chennai, Tamil Nadu -...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 19...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1527847268000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>Prashant Duble Territory Sales Manager - Bhart...</td>\n",
       "      <td>[{'label': ['College'], 'points': [{'start': 5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1540021594000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>Rayees Parwez Store Manager - Sales &amp; Operatio...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 48...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1539697381000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>Sachin Kushwah Territory Sales Manager - Essar...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1540021512000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>Sakshi Sundriyal Manager- Sales - Benefits and...</td>\n",
       "      <td>[{'label': ['Links'], 'points': [{'start': 627...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1539697642000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Vijat Kumar Area sales Manager in Syska Lights...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1540021607000, 'last_updated...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>701 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  \\\n",
       "0    Afreen Jamadar Active member of IIIT Committee...   \n",
       "1    Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
       "2    Anvitha Rao Automation developer  - Email me o...   \n",
       "3    arjun ks Senior Program coordinator - oracle I...   \n",
       "4    Arun Elumalai QA Tester  Chennai, Tamil Nadu -...   \n",
       "..                                                 ...   \n",
       "696  Prashant Duble Territory Sales Manager - Bhart...   \n",
       "697  Rayees Parwez Store Manager - Sales & Operatio...   \n",
       "698  Sachin Kushwah Territory Sales Manager - Essar...   \n",
       "699  Sakshi Sundriyal Manager- Sales - Benefits and...   \n",
       "700  Vijat Kumar Area sales Manager in Syska Lights...   \n",
       "\n",
       "                                            annotation  extras  \\\n",
       "0    [{'label': ['Email Address'], 'points': [{'sta...     NaN   \n",
       "1    [{'label': ['Skills'], 'points': [{'start': 80...     NaN   \n",
       "2    [{'label': ['Links'], 'points': [{'start': 288...     NaN   \n",
       "3    [{'label': ['Skills'], 'points': [{'start': 50...     NaN   \n",
       "4    [{'label': ['Skills'], 'points': [{'start': 19...     NaN   \n",
       "..                                                 ...     ...   \n",
       "696  [{'label': ['College'], 'points': [{'start': 5...     NaN   \n",
       "697  [{'label': ['Skills'], 'points': [{'start': 48...     NaN   \n",
       "698  [{'label': ['Skills'], 'points': [{'start': 37...     NaN   \n",
       "699  [{'label': ['Links'], 'points': [{'start': 627...     NaN   \n",
       "700                                               None     NaN   \n",
       "\n",
       "                                              metadata  \n",
       "0    {'first_done_at': 1527844872000, 'last_updated...  \n",
       "1    {'first_done_at': 1527845028000, 'last_updated...  \n",
       "2    {'first_done_at': 1527744637000, 'last_updated...  \n",
       "3    {'first_done_at': 1527834843000, 'last_updated...  \n",
       "4    {'first_done_at': 1527847268000, 'last_updated...  \n",
       "..                                                 ...  \n",
       "696  {'first_done_at': 1540021594000, 'last_updated...  \n",
       "697  {'first_done_at': 1539697381000, 'last_updated...  \n",
       "698  {'first_done_at': 1540021512000, 'last_updated...  \n",
       "699  {'first_done_at': 1539697642000, 'last_updated...  \n",
       "700  {'first_done_at': 1540021607000, 'last_updated...  \n",
       "\n",
       "[701 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = trim_entity_spans(convert_dataturks_to_spacy(\"//Users/cosmos/Desktop/Deepblue/DeepBlue/BACKEND/ML/Datasets (UNCLEANED)/NER_RESUME-300.json\"))\n",
    "#data = trim_entity_spans(convert_dataturks_to_spacy(\"/Users/cosmos/Desktop/Deepblue/ref - kaggle/pre_trained.json\"))\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_entities(training_data):\n",
    "    \n",
    "#     clean_data = []\n",
    "#     for text, annotation in training_data:\n",
    "        \n",
    "#         entities = annotation.get('entities')\n",
    "#         entities_copy = entities.copy()\n",
    "        \n",
    "#         # append entity only if it is longer than its overlapping entity\n",
    "#         i = 0\n",
    "#         for entity in entities_copy:\n",
    "#             j = 0\n",
    "#             for overlapping_entity in entities_copy:\n",
    "#                 # Skip self\n",
    "#                 if i != j:\n",
    "#                     e_start, e_end, oe_start, oe_end = entity[0], entity[1], overlapping_entity[0], overlapping_entity[1]\n",
    "#                     # Delete any entity that overlaps, keep if longer\n",
    "#                     if ((e_start >= oe_start and e_start <= oe_end) \\\n",
    "#                     or (e_end <= oe_end and e_end >= oe_start)) \\\n",
    "#                     and ((e_end - e_start) <= (oe_end - oe_start)):\n",
    "#                         entities.remove(entity)\n",
    "#                 j += 1\n",
    "#             i += 1\n",
    "#         clean_data.append((text, {'entities': entities}))\n",
    "                \n",
    "#     return clean_data\n",
    "\n",
    "# data = clean_entities(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Changing data to appropriate format so as to feed it to the model\n",
    "# import nltk\n",
    "# #nltk.download('stopwords')\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# en_stops = set(stopwords.words('english'))\n",
    "\n",
    "# df_data = pd.DataFrame(columns = [\"clean_content\", \"entities_mapped\"])\n",
    "\n",
    "# entities_mapped = []\n",
    "# clean_content = []\n",
    "\n",
    "# for i in range(len(data)):\n",
    "#     content = data[i][0].split()\n",
    "#     entities = data[i][1][\"entities\"]\n",
    "#     words = []\n",
    "#     labels = []\n",
    "    \n",
    "#     for word in content:\n",
    "#         if (word.isalnum() or word.find(\".com\") != -1) and word not in en_stops:\n",
    "#             words.append(word)\n",
    "#             found = False\n",
    "#             for entity in sorted(entities):\n",
    "#                 ent_start = entity[0]\n",
    "#                 ent_end = entity[1]\n",
    "#                 ent_label = entity[2]\n",
    "\n",
    "#                 if word in data[i][0][ent_start: ent_end].split(): \n",
    "#                     labels.append(ent_label)\n",
    "#                     found = True\n",
    "#                     break\n",
    "#             if not found:\n",
    "#                 labels.append(\"O\")\n",
    "    \n",
    "#     entities_mapped.append(labels)\n",
    "#     clean_content.append(words)\n",
    "    \n",
    "    \n",
    "# df_data[\"entities_mapped\"] = entities_mapped\n",
    "# df_data[\"clean_content\"] = clean_content\n",
    "# df_data[\"clean_content\"] = df_data[\"clean_content\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "EPOCHS = 6\n",
    "# NUM_LABELS = 12\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = '/Users/cosmos/Documents/BERT/Final-HF/'\n",
    "#STATE_DICT = torch.load('/Users/cosmos/Documents/uncased_L-12_H-768_A-12/results/model_e10.tar', map_location=DEVICE)\n",
    "TOKENIZER = BertTokenizerFast('/Users/cosmos/Documents/BERT/Final-HF/vocab.txt', lowercase=True)\n",
    "#MODEL = BertForTokenClassification.from_pretrained(MODEL_PATH, state_dict=STATE_DICT['model_state_dict'], num_labels=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(offset, labels):\n",
    "    if offset[0] == 0 and offset[1] == 0:\n",
    "        return 'O'\n",
    "    for label in labels:\n",
    "        if offset[1] >= label[0] and offset[0] <= label[1]:\n",
    "            return label[2]\n",
    "    return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = [\"UNKNOWN\",\"Address\",\"Links\",\"University\",\"Rewards and Achievements\", \"O\", \"Name\", \"Degree\",\"Skills\",\"College Name\",\"Email Address\",\"Designation\",\"Companies worked at\",\"Graduation Year\",\"Years of Experience\",\"Location\"]\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i:t for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume(data, tokenizer, tag2idx, max_len, is_test=False):\n",
    "    tok = tokenizer.encode_plus(data[0], max_length=max_len, return_offsets_mapping=True)\n",
    "    curr_sent = {'orig_labels':[], 'labels': []}\n",
    "    \n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    \n",
    "    if not is_test:\n",
    "        labels = data[1]['entities']\n",
    "        labels.reverse()\n",
    "        for off in tok['offset_mapping']:\n",
    "            label = get_label(off, labels)\n",
    "            curr_sent['orig_labels'].append(label)\n",
    "            curr_sent['labels'].append(tag2idx[label])\n",
    "        curr_sent['labels'] = curr_sent['labels'] + ([0] * padding_length)\n",
    "    \n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + ([0] * padding_length)\n",
    "    return curr_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeDataset(Dataset):\n",
    "    def __init__(self, resume, tokenizer, tag2idx, max_len, is_test=False):\n",
    "        self.resume = resume\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.resume)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = process_resume(self.resume[idx], self.tokenizer, self.tag2idx, self.max_len, self.is_test)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(data['input_ids'], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(data['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(data['labels'], dtype=torch.long),\n",
    "            'orig_label': data['orig_labels']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(data)\n",
    "train_data, val_data = data[:180], data[180:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = ResumeDataset(train_data, TOKENIZER, tag2idx, MAX_LEN)\n",
    "val_d = ResumeDataset(val_data, TOKENIZER, tag2idx, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_d)\n",
    "train_dl = DataLoader(train_d, sampler=train_sampler, batch_size=8,collate_fn=lambda x: x )\n",
    "#dataiter = iter(train_dl)\n",
    "#data = dataiter.next()\n",
    "val_dl = DataLoader(val_d, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(model, ff):\n",
    "\n",
    "    # ff: full_finetuning\n",
    "    if ff:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters())\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_tokens(tokenizer, tag2idx):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    pad_tok = vocab[\"[PAD]\"]\n",
    "    sep_tok = vocab[\"[SEP]\"]\n",
    "    cls_tok = vocab[\"[CLS]\"]\n",
    "    o_lab = tag2idx[\"O\"]\n",
    "\n",
    "    return pad_tok, sep_tok, cls_tok, o_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot_confusion_matrix(valid_tags, pred_tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Create an annotated confusion matrix by adding label\n",
    "    annotations and formatting to sklearn's `confusion_matrix`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create header from unique tags\n",
    "    header = sorted(list(set(valid_tags + pred_tags)))\n",
    "\n",
    "    # Calculate the actual confusion matrix\n",
    "    matrix = confusion_matrix(valid_tags, pred_tags, labels=header)\n",
    "\n",
    "    # Final formatting touches for the string output\n",
    "    mat_formatted = [header[i] + \"\\t\\t\\t\" + str(row) for i, row in enumerate(matrix)]\n",
    "    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(valid_tags, pred_tags):\n",
    "    return (np.array(valid_tags) == np.array(pred_tags)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GRAD_NORM = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/cosmos/Documents/BERT/Final-HF/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /Users/cosmos/Documents/BERT/Final-HF/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(MODEL_PATH, num_labels=len(tag2idx))\n",
    "model.to(DEVICE);\n",
    "optimizer_grouped_parameters = get_hyperparameters(model, True)\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    idx2tag,\n",
    "    tag2idx,\n",
    "    max_grad_norm,\n",
    "    device,\n",
    "    train_dataloader,\n",
    "    valid_dataloader\n",
    "):\n",
    "\n",
    "    pad_tok, sep_tok, cls_tok, o_lab = get_special_tokens(tokenizer, tag2idx)\n",
    "    \n",
    "    epoch = 0\n",
    "    val_acc=[]\n",
    "    ep=[]\n",
    "    val_loss=[]\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        epoch += 1\n",
    "\n",
    "        # Training loop\n",
    "        print(\"Starting training loop.\")\n",
    "        model.train()\n",
    "        tr_loss, tr_accuracy = 0, 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        tr_preds, tr_labels = [], []\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Add batch to gpu\n",
    "            \n",
    "            #batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            loss, tr_logits = outputs[:2]\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Compute train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "            preds_mask = (\n",
    "                (b_input_ids != cls_tok)\n",
    "                & (b_input_ids != pad_tok)\n",
    "                & (b_input_ids != sep_tok)\n",
    "            )\n",
    "\n",
    "            tr_logits = tr_logits.cpu().detach().numpy()\n",
    "            tr_label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "            preds_mask = preds_mask.cpu().detach().numpy()\n",
    "            tr_batch_preds = np.argmax(tr_logits[preds_mask.squeeze()], axis=1)\n",
    "            tr_batch_labels = tr_label_ids.to(\"cpu\").numpy()\n",
    "            tr_preds.extend(tr_batch_preds)\n",
    "            tr_labels.extend(tr_batch_labels)\n",
    "\n",
    "            # Compute training accuracy\n",
    "            tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n",
    "            tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                parameters=model.parameters(), max_norm=max_grad_norm\n",
    "            )\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        tr_loss = tr_loss / nb_tr_steps\n",
    "        tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "\n",
    "        # Print training loss and accuracy per epoch\n",
    "        print(f\"Train loss: {tr_loss}\")\n",
    "        print(f\"Train accuracy: {tr_accuracy}\")\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Validation loop\n",
    "        \"\"\" \n",
    "        print(\"Starting validation loop.\")\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        predictions, true_labels = [], []\n",
    "\n",
    "        for batch in valid_dataloader:\n",
    "\n",
    "            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    labels=b_labels,\n",
    "                )\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "            preds_mask = (\n",
    "                (b_input_ids != cls_tok)\n",
    "                & (b_input_ids != pad_tok)\n",
    "                & (b_input_ids != sep_tok)\n",
    "            )\n",
    "\n",
    "            logits = logits.cpu().detach().numpy()\n",
    "            label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "            preds_mask = preds_mask.cpu().detach().numpy()\n",
    "            val_batch_preds = np.argmax(logits[preds_mask.squeeze()], axis=1)\n",
    "            val_batch_labels = label_ids.to(\"cpu\").numpy()\n",
    "            predictions.extend(val_batch_preds)\n",
    "            true_labels.extend(val_batch_labels)\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(val_batch_labels, val_batch_preds)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        # Evaluate loss, acc, conf. matrix, and class. report on devset\n",
    "        pred_tags = [idx2tag[i] for i in predictions]\n",
    "        valid_tags = [idx2tag[i] for i in true_labels]\n",
    "        cl_report = classification_report([valid_tags], [pred_tags])\n",
    "        conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "\n",
    "        # Report metrics\n",
    "        print(f\"Validation loss: {eval_loss}\")\n",
    "        print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "        print(f\"Classification Report:\\n {cl_report}\")\n",
    "        print(f\"Confusion Matrix:\\n {conf_mat}\")\n",
    "        val_acc.append(eval_accuracy)\n",
    "        ep.append(epoch)\n",
    "        val_loss.append(eval_loss)\n",
    "    \n",
    "    plt.plot(ep, val_acc, 'g', label='Validation accuracy')\n",
    "    #plt.plot(ep, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Validation acuuracy ')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "21",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3082\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 21",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_19095/3275262570.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_and_save_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mTOKENIZER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_19095/3852508502.py\u001b[0m in \u001b[0;36mtrain_and_save_model\u001b[0;34m(model, tokenizer, optimizer, epochs, idx2tag, tag2idx, max_grad_norm, device, train_dataloader, valid_dataloader)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtr_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Add batch to gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_19095/4125384172.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         return {\n\u001b[1;32m     15\u001b[0m             \u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3081\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 21"
     ]
    }
   ],
   "source": [
    "train_and_save_model(\n",
    "    model, \n",
    "    TOKENIZER, \n",
    "    optimizer, \n",
    "    EPOCHS, \n",
    "    idx2tag, \n",
    "    tag2idx, \n",
    "    MAX_GRAD_NORM, \n",
    "    DEVICE, \n",
    "    train_dl, \n",
    "    val_dl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PyTorch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_35597/3597352979.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'PyTorch' is not defined"
     ]
    }
   ],
   "source": [
    "PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume(text, tokenizer, max_len):\n",
    "    tok = tokenizer.encode_plus(text, max_length=max_len, return_offsets_mapping=True)\n",
    "    \n",
    "    curr_sent = dict()\n",
    "    \n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "        \n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + ([0] * padding_length)\n",
    "    \n",
    "    final_data = {\n",
    "        'input_ids': torch.tensor(curr_sent['input_ids'], dtype=torch.long),\n",
    "        'token_type_ids': torch.tensor(curr_sent['token_type_ids'], dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(curr_sent['attention_mask'], dtype=torch.long),\n",
    "        'offset_mapping': tok['offset_mapping']\n",
    "    }\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\",\"Skills\",\"College Name\",\"Email Address\",\"Designation\",\"Companies worked at\",\"Graduation Year\",\"Years of Experience\",\"Location\"]\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i:t for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, idx2tag, tag2idx, device, test_resume):\n",
    "    model.eval()\n",
    "    data = process_resume(test_resume, tokenizer, MAX_LEN)\n",
    "    input_ids, input_mask = data['input_ids'].unsqueeze(0), data['attention_mask'].unsqueeze(0)\n",
    "    labels = torch.tensor([1] * input_ids.size(0), dtype=torch.long).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=input_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "    \n",
    "    logits = logits.cpu().detach().numpy()\n",
    "    label_ids = np.argmax(logits, axis=2)\n",
    "    \n",
    "    entities = []\n",
    "    for label_id, offset in zip(label_ids[0], data['offset_mapping']):\n",
    "        curr_id = idx2tag[label_id]\n",
    "        curr_start = offset[0]\n",
    "        curr_end = offset[1]\n",
    "        if curr_id != 'O':\n",
    "            if len(entities) > 0 and entities[-1]['entity'] == curr_id and curr_start - entities[-1]['end'] in [0, 1]:\n",
    "                entities[-1]['end'] = curr_end\n",
    "            else:\n",
    "                entities.append({'entity': curr_id, 'start': curr_start, 'end':curr_end})\n",
    "    for ent in entities:\n",
    "        ent['text'] = test_resume[ent['start']:ent['end']]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = data[41][0]\n",
    "entities = predict(model, TOKENIZER, idx2tag, tag2idx, DEVICE, resume)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
