{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned trailing spaces\n",
      "693\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    print(\"Cleaned trailing spaces\")\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "\n",
    "def for_spacy(input_file):\n",
    "    count = 0\n",
    "    class_labels = list()\n",
    "    training_data = []\n",
    "    lines = []\n",
    "    with open(input_file, 'r')  as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        entities = []\n",
    "        if  data['annotation'] == None:\n",
    "            continue\n",
    "        text = data['content']\n",
    "        count += 1\n",
    "        for annotation in data['annotation']:\n",
    "            point = annotation['points'][0]\n",
    "            labels = annotation['label']\n",
    "            \n",
    "            for lb in labels:\n",
    "                if lb not in class_labels:\n",
    "                    class_labels.append(lb)\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "\n",
    "            for label in labels:\n",
    "                entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "        training_data.append((text, {\"entities\" : entities}))\n",
    "    training_data = trim_entity_spans(training_data)\n",
    "    #print(training_data)\n",
    "    print(count)\n",
    "    \n",
    "    output_file = '/Users/cosmos/Desktop/DeepBlue/BACKEND/ML/Spacy - Model/700/d.pkl'\n",
    "    with open(output_file, 'wb') as fp:\n",
    "            pickle.dump(training_data, fp)\n",
    "    class_labels = str(class_labels)\n",
    "    output_file = '/Users/cosmos/Desktop/DeepBlue/BACKEND/ML/Spacy - Model/700/labels.txt'\n",
    "    with open(output_file, 'w') as fp:\n",
    "            fp.write(class_labels)\n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for_spacy('/Users/cosmos/Documents/resume_parser_nlp-master/commons/Models/training/training_data/traindata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n",
      "Created a blank en model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.ner.EntityRecognizer object at 0x29960d2e0> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_36870/1595246015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mcustom_nlp_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/cosmos/Desktop/DeepBlue/BACKEND/ML/Spacy - Model/700/d.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_36870/1595246015.py\u001b[0m in \u001b[0;36mcustom_nlp_train\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'ner'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Added ner pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mbad_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE966\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.ner.EntityRecognizer object at 0x29960d2e0> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import spacy\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding \n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "#LABEL = ['Email Address', 'Links', 'Skills', 'Graduation Year', 'College Name', 'Degree', 'Companies worked at', 'Location', 'Name', 'Designation', 'projects', 'Years of Experience', 'Can Relocate to', 'UNKNOWN', 'Rewards and Achievements', 'Address', 'University', 'Relocate to', 'Certifications', 'state', 'links', 'College', 'training', 'des', 'abc']\n",
    "\n",
    "def custom_nlp_train(filename):\n",
    "    with open(filename, 'rb') as fp:\n",
    "        doc = pickle.load(fp)\n",
    "    \n",
    "    ## Creating a blank spacy model\n",
    "    nlp = spacy.blank('en')\n",
    "    print(\"Created a blank en model\")\n",
    "\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "        print(\"Added ner pipe\")\n",
    "    else:\n",
    "        ner.get_pipe('ner')\n",
    "\n",
    "    # for i in LABEL:\n",
    "    #     ner.add_label(i)\n",
    "\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in doc:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    optimizer = nlp.begin_training()\n",
    "    \n",
    "    for itn in range(30):\n",
    "                random.shuffle(doc)\n",
    "                losses = {}\n",
    "                # batch up the examples using spaCy's minibatch\n",
    "                batches = minibatch(doc, size=compounding(4.0, 32.0, 1.001))\n",
    "                for batch in batches:\n",
    "                    texts, annotations = zip(*batch)\n",
    "                    nlp.update(\n",
    "                        texts,  # batch of texts\n",
    "                        annotations,  # batch of annotations\n",
    "                        drop=0.3,  # dropout - make it harder to memorise data\n",
    "                        losses=losses,\n",
    "                    )\n",
    "                print(\"Losses\", losses)\n",
    "\n",
    "    output_dir = os.path.dirname(os.path.realpath(__file__)) + \"/training_data\"\n",
    "    new_model_name=\"customspacy\"\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "\n",
    "    nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)           \n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    custom_nlp_train('/Users/cosmos/Desktop/DeepBlue/BACKEND/ML/Spacy - Model/700/d.pkl')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
