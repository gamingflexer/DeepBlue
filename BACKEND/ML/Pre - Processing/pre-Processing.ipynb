{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tika\n",
      "  Downloading tika-1.24.tar.gz (28 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tika) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from tika) (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (2020.12.5)\n",
      "Building wheels for collected packages: tika\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32884 sha256=b029bafb909f7af8cdd06b8c9d5ffd6b51f159a7c99dedc8aa0ede08fb6c40e5\n",
      "  Stored in directory: c:\\users\\kishor wagh\\appdata\\local\\pip\\cache\\wheels\\75\\66\\8b\\d1acbac7d49f3d98ade76c51ae5d72cec1866131a3b1ad9f82\n",
      "Successfully built tika\n",
      "Installing collected packages: tika\n",
      "Successfully installed tika-1.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import tika\n",
    "from tika import parser # pip install tika"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visulizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-89e307fff7dd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-89e307fff7dd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    docker run -d -p 9998:9998 logicalspark/docker-tikaserver\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "docker run -d -p 9998:9998 logicalspark/docker-tikaserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 20:17:45,968 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to C:\\Users\\KISHOR~1\\AppData\\Local\\Temp\\tika-server.jar.md5.\n",
      "2022-01-17 20:17:47,717 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to C:\\Users\\KISHOR~1\\AppData\\Local\\Temp\\tika-server.jar.\n",
      "2022-01-17 20:18:44,817 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Resume\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "Contact\n",
      "\n",
      "www.linkedin.com/in/aju-\n",
      "palleri-248798a4 (LinkedIn)\n",
      "\n",
      "Top Skills\n",
      "Teaching\n",
      "Core Java\n",
      "Python (Programming Language)\n",
      "\n",
      "Certifications\n",
      "Use WordPress to Create a Blog for\n",
      "your Business\n",
      "Create Your First Web App with\n",
      "Python and Flask\n",
      "Excel Skills for Business: Essentials\n",
      "Introduction to Relational Database\n",
      "and SQL\n",
      "Build a Full Website using\n",
      "WordPress\n",
      "\n",
      "Aju Palleri\n",
      "Assistant Professor at PCE, New Panvel\n",
      "Mumbai\n",
      "\n",
      "Summary\n",
      "I am working as an Assistant Professor with PCE, New Panvel for\n",
      "past 10 years. I have keen interest in programming and learning\n",
      "new languages. C, C++, Java, Python, DBMS, Digital Logic Design,\n",
      "Computer Simulation and modelling  and web programming are\n",
      "subjects taught to students.\n",
      "\n",
      "I have mentored students to National Level Smart India Hackathon\n",
      "and Deep Blue Competition. I have guided SE, TE and BE students\n",
      "in their projects and it has been fun to bring in creative ideas and\n",
      "develop using different technologies I am part of internal software\n",
      "development team, responsible to develop software that maintains\n",
      "academic records of students.\n",
      "\n",
      "Experience\n",
      "\n",
      "Pillai College of Engineering\n",
      "Assistant Professor at PCE, New Panvel\n",
      "August 2011 - Present (10 years 6 months)\n",
      "Navi Mumbai\n",
      "\n",
      "AC Nielsen\n",
      "DA Junior Programmer\n",
      "January 2011 - July 2011 (7 months)\n",
      "Byculla, Mumbai\n",
      "\n",
      "Creating surveys using Java Script and confirmit tool.\n",
      "\n",
      "Education\n",
      "Pillai College of Engineering\n",
      "Master's degree, Information Technology · (July 2012 - August 2015)\n",
      "\n",
      "Bharati Vidyapeeth's College of Engineering\n",
      "Bachelor in Engineering, Information Technology · (July 2004 - May 2008)\n",
      "\n",
      "  Page 1 of 2\n",
      "\n",
      "https://www.linkedin.com/in/aju-palleri-248798a4?jobid=1234&lipi=urn%3Ali%3Apage%3Ad_jobs_easyapply_pdfgenresume%3BWezi4k4uTPKgYQqnqv7u9g%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_jobs_easyapply_pdfgenresume-v02_profile\n",
      "https://www.linkedin.com/in/aju-palleri-248798a4?jobid=1234&lipi=urn%3Ali%3Apage%3Ad_jobs_easyapply_pdfgenresume%3BWezi4k4uTPKgYQqnqv7u9g%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_jobs_easyapply_pdfgenresume-v02_profile\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "  Page 2 of 2\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Method 1 \n",
    "#os.environ['TIKA_SERVER_JAR'] = 'https://repo1.maven.org/maven2/org/apache/tika/tika-server/1.19/tika-server-1.19.jar'\n",
    "#tika.initVM()\n",
    "raw = parser.from_file(r\"C:\\Users\\KISHOR WAGH\\Documents\\GitHub\\DeepBlue\\BACKEND\\File_conversion-OCR\\Resumes\\resume_linkdien.pdf\")\n",
    "print(raw['content'])\n",
    "text = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement textract==1.12.0\n",
      "ERROR: No matching distribution found for textract==1.12.0\n"
     ]
    }
   ],
   "source": [
    "pip install textract==1.12.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ShellError",
     "evalue": "The command `pdftotext C:\\Users\\KISHOR WAGH\\Documents\\GitHub\\DeepBlue\\BACKEND\\File_conversion-OCR\\Resumes\\resume_linkdien.pdf -` failed with exit code 127\n------------- stdout -------------\n------------- stderr -------------\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             pipe = subprocess.Popen(\n\u001b[0m\u001b[0;32m     88\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1310\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1311\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1312\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mShellError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-3f515c101ec3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Method 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtextract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\KISHOR WAGH\\Documents\\GitHub\\DeepBlue\\BACKEND\\File_conversion-OCR\\Resumes\\resume_linkdien.pdf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#text = textract.process('/Users/cosmos/Desktop/Deepblue/DeepBlue/Dataset-Scraping/Resumes/Word Doc Resume #6.docx')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\textract\\parsers\\__init__.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(filename, input_encoding, output_encoding, extension, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiletype_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, filename, input_encoding, output_encoding, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# output encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# http://nedbatchelder.com/text/unipain/unipain.html#35\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mbyte_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0municode_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0municode_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\textract\\parsers\\pdf_parser.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(self, filename, method, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_pdfminer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pdfminer'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\textract\\parsers\\pdf_parser.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(self, filename, method, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pdftotext'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_pdftotext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mShellError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;31m# If pdftotext isn't installed and the pdftotext method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\textract\\parsers\\pdf_parser.py\u001b[0m in \u001b[0;36mextract_pdftotext\u001b[1;34m(self, filename, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'pdftotext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[1;31m# File not found.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;31m# This is equivalent to getting exitcode 127 from sh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                 raise exceptions.ShellError(\n\u001b[0m\u001b[0;32m     96\u001b[0m                     \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m127\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 )\n",
      "\u001b[1;31mShellError\u001b[0m: The command `pdftotext C:\\Users\\KISHOR WAGH\\Documents\\GitHub\\DeepBlue\\BACKEND\\File_conversion-OCR\\Resumes\\resume_linkdien.pdf -` failed with exit code 127\n------------- stdout -------------\n------------- stderr -------------\n"
     ]
    }
   ],
   "source": [
    "#Method 2 \n",
    "import textract\n",
    "text = textract.process(r\"C:\\Users\\KISHOR WAGH\\Documents\\GitHub\\DeepBlue\\BACKEND\\File_conversion-OCR\\Resumes\\resume_linkdien.pdf\")\n",
    "#text = textract.process('/Users/cosmos/Desktop/Deepblue/DeepBlue/Dataset-Scraping/Resumes/Word Doc Resume #6.docx')\n",
    "text = str(text)\n",
    "text=re.sub(\"\\n\", \" \",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre - Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.linkedin.com/in/aju-palleri-248798a4?jobid=1234&lipi=urn%3Ali%3Apage%3Ad_jobs_easyapply_pdfgenresume%3BWezi4k4uTPKgYQqnqv7u9g%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_jobs_easyapply_pdfgenresume-v02_profile'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#links\n",
    "url = re.search(\"(?P<url>https?://[^\\s]+)\", text).group(\"url\")\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contact@tutorialspoint.com', 'feedback@tp.com']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting Email & numbers\n",
    "\n",
    "extracted_text={}\n",
    "\n",
    "\n",
    "emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", text)\n",
    "\n",
    "def get_phone_numbers(string):\n",
    "    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "    phone_numbers = r.findall(string)\n",
    "    return [re.sub(r'\\D', '', num) for num in phone_numbers]\n",
    "\n",
    "#finding them\n",
    "phone_number= get_phone_numbers(text)\n",
    "\n",
    "#extracted_text['E-mail'] = email\n",
    "extracted_text['Phone number'] = phone_number\n",
    "\n",
    "phone_number\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi i am obama davis .Please contact us at contact@tutorialspoint.com for further information. name is Om Surve from India.You can also give feedback at feedback@tp.com, My number is 9869249403, page 1 of 1, date : 10-1-2022, 10/10/2121\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi i am obama davis .please contact us at contact for further information. name is om surve from india.you can also give feedback at feedback my number is , , date : 10-1-2022, 10/10/2121'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_stop = [\"page 1 of 1\",\"Resume\", \"page 1 of 2\",\"page 1 of 3\", \"page 1 of 4\", \n",
    "                \"page 2 of 2\",\"page 3 of 3\",\"page 4 of 4\",\"page 2 of 3\",\n",
    "                \"page 2 of 4\",\"page 3 of 4\",\"resume\"]\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = list(stop_words.union(words_stop))\n",
    "\n",
    "def pre_process(text):\n",
    "    text = \"\".join(text.split('\\n')) #remove whitespaces\n",
    "    text = text.lower()\n",
    "    \n",
    "    #using re\n",
    "    text=re.sub('http\\S+\\s*',' ',text)\n",
    "    text=re.sub('RT|cc',' ',text)\n",
    "    text=re.sub('#\\S+',' ',text)\n",
    "    text=re.sub('@\\S+',' ',text)\n",
    "    \n",
    "    for i in range (len(emails)): #removes emails\n",
    "        text = text.replace(emails[i],\"\") \n",
    "    \n",
    "    text = re.sub(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})','',text) #removes phone numbers\n",
    "    text=re.sub(r'[^\\x00-\\x7f]',' ',text)\n",
    "    text=re.sub('\\s+',' ',text)\n",
    "    text=re.sub(\"\\n\", \" \",text)\n",
    "    \n",
    "    #remove uncessary stop words\n",
    "    for i in range (len(words_stop)): #removes emails\n",
    "        text = text.replace(words_stop[i],\"\") \n",
    "    \n",
    "    return ''.join(text)\n",
    "\n",
    "\n",
    "pre_process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numbers\n",
    "\n",
    "#re.sub(r'\\d+','',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the first two maketrans arguments must have equal length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_70532/1730078248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_70532/1730078248.py\u001b[0m in \u001b[0;36mrp\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtranslator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: the first two maketrans arguments must have equal length"
     ]
    }
   ],
   "source": [
    "#remove punctaions\n",
    "\n",
    "def rp(text):\n",
    "    translator=str.maketrans(\",\",string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "rp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing deauflt stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words= set(stopwords.words(\"english\"))\n",
    "    word_tokens=word_tokenize(text)\n",
    "    new_text= [word for word in word_tokens if word not in stopwords]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_15869/1150756350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mstemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#stem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "#stemming is it necsaary?\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer1 = SnowballStemmer(language='english')\n",
    "\n",
    "def stemmer(text):\n",
    "    word_tokens=word_tokenize(text)\n",
    "    stems = [stemmer1.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "stemmer(text)\n",
    "#stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'Entry\",\n",
       " 'Level',\n",
       " 'Resume',\n",
       " '(',\n",
       " 'Delete',\n",
       " 'this',\n",
       " 'tag',\n",
       " ')',\n",
       " '\\\\n\\\\n\\\\n\\\\nAlex',\n",
       " 'Carter\\\\n\\\\nPosition',\n",
       " 'Title\\\\n\\\\n\\\\n\\\\nPhone',\n",
       " 'Number\\\\n\\\\n\\\\n\\\\nLocation\\\\n\\\\n\\\\n\\\\nname',\n",
       " '@',\n",
       " 'email.com\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nEDUCATION\\\\n\\\\n\\\\n\\\\nDegree',\n",
       " 'Title',\n",
       " ',',\n",
       " 'University\\\\n\\\\nGRADUATION',\n",
       " 'YEAR',\n",
       " ',',\n",
       " 'GPA\\\\n\\\\nLatin',\n",
       " 'Honors',\n",
       " 'Earned',\n",
       " '\\\\nInclude',\n",
       " 'Scholarship',\n",
       " '\\\\nDid',\n",
       " 'you',\n",
       " 'study',\n",
       " 'abroad',\n",
       " '?',\n",
       " '\\\\nInclude',\n",
       " 'that',\n",
       " 'here',\n",
       " '..',\n",
       " '\\\\n\\\\n\\\\n\\\\nTECHNICAL',\n",
       " 'SKILLS\\\\n\\\\n\\\\n\\\\nThis',\n",
       " 'be',\n",
       " 'where',\n",
       " 'you',\n",
       " 'will',\n",
       " 'list',\n",
       " 'all',\n",
       " 'of',\n",
       " '\\\\nthe',\n",
       " 'program',\n",
       " 'and',\n",
       " 'software',\n",
       " 'that',\n",
       " 'you',\n",
       " 'be',\n",
       " 'familiar',\n",
       " 'with',\n",
       " 'using.\\\\n\\\\n\\\\n\\\\nList',\n",
       " 'Item',\n",
       " '1\\\\n\\\\nList',\n",
       " 'Item',\n",
       " '2\\\\n\\\\nList',\n",
       " 'Item',\n",
       " '3\\\\n\\\\nList',\n",
       " 'Item',\n",
       " '4\\\\n\\\\nList',\n",
       " 'Item',\n",
       " '5\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSUMMARY\\\\n\\\\n\\\\n\\\\nUse',\n",
       " 'this',\n",
       " 'space',\n",
       " 'to',\n",
       " 'write',\n",
       " 'a',\n",
       " 'two',\n",
       " 'or',\n",
       " 'three',\n",
       " 'sentence',\n",
       " 'summary',\n",
       " 'of',\n",
       " 'who',\n",
       " 'you',\n",
       " 'be',\n",
       " 'as',\n",
       " 'a',\n",
       " 'professional',\n",
       " '.',\n",
       " 'Include',\n",
       " 'that',\n",
       " 'you',\n",
       " 'be',\n",
       " 'an',\n",
       " 'entry',\n",
       " 'level',\n",
       " 'employee',\n",
       " ',',\n",
       " 'and',\n",
       " 'because',\n",
       " 'you',\n",
       " 'be',\n",
       " 'entry',\n",
       " 'level',\n",
       " 'take',\n",
       " 'this',\n",
       " 'as',\n",
       " 'an',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'mention',\n",
       " 'who',\n",
       " 'you',\n",
       " 'be',\n",
       " 'as',\n",
       " 'an',\n",
       " 'employee',\n",
       " '.',\n",
       " 'Include',\n",
       " 'whatever',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'your',\n",
       " 'industry',\n",
       " 'that',\n",
       " 'you',\n",
       " 'do',\n",
       " 'have',\n",
       " '.',\n",
       " '\\\\n\\\\nSee',\n",
       " 'my',\n",
       " 'personal',\n",
       " 'website',\n",
       " 'and',\n",
       " 'blog',\n",
       " 'at',\n",
       " 'www.YourWebsiteHere.com\\\\n\\\\n\\\\n\\\\nEXPERIENCE\\\\n\\\\n\\\\nYour',\n",
       " 'Job',\n",
       " 'Title',\n",
       " 'Here',\n",
       " '2020',\n",
       " '-',\n",
       " 'PRESENT',\n",
       " '\\\\n\\\\nCompany',\n",
       " 'Name',\n",
       " ',',\n",
       " 'Location\\\\n\\\\nWhether',\n",
       " 'this',\n",
       " 'position',\n",
       " 'be',\n",
       " 'an',\n",
       " 'internship',\n",
       " ',',\n",
       " 'part-time',\n",
       " ',',\n",
       " 'or',\n",
       " 'full-time',\n",
       " 'position',\n",
       " ',',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'treat',\n",
       " 'the',\n",
       " 'same',\n",
       " '.',\n",
       " 'This',\n",
       " 'be',\n",
       " 'where',\n",
       " 'you',\n",
       " 'write',\n",
       " 'your',\n",
       " 'responsibilities',\n",
       " 'in',\n",
       " 'this',\n",
       " 'position',\n",
       " 'and',\n",
       " 'how',\n",
       " 'your',\n",
       " 'action',\n",
       " 'in',\n",
       " 'your',\n",
       " 'role',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'overall',\n",
       " 'success',\n",
       " 'for',\n",
       " 'the',\n",
       " 'company.\\\\n\\\\n\\\\n\\\\nYour',\n",
       " 'Job',\n",
       " 'Title',\n",
       " 'Here',\n",
       " '2020',\n",
       " '-',\n",
       " 'PRESENT',\n",
       " '\\\\n\\\\nCompany',\n",
       " 'Name',\n",
       " ',',\n",
       " 'Location\\\\n\\\\nWhether',\n",
       " 'this',\n",
       " 'position',\n",
       " 'be',\n",
       " 'an',\n",
       " 'internship',\n",
       " ',',\n",
       " 'part-time',\n",
       " ',',\n",
       " 'or',\n",
       " 'full-time',\n",
       " 'position',\n",
       " ',',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'treat',\n",
       " 'the',\n",
       " 'same',\n",
       " '.',\n",
       " 'This',\n",
       " 'be',\n",
       " 'where',\n",
       " 'you',\n",
       " 'write',\n",
       " 'your',\n",
       " 'responsibilities',\n",
       " 'in',\n",
       " 'this',\n",
       " 'position',\n",
       " 'and',\n",
       " 'how',\n",
       " 'your',\n",
       " 'action',\n",
       " 'in',\n",
       " 'your',\n",
       " 'role',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'overall',\n",
       " 'success',\n",
       " 'for',\n",
       " 'the',\n",
       " 'company.\\\\n\\\\n\\\\n\\\\nYour',\n",
       " 'Job',\n",
       " 'Title',\n",
       " 'Here',\n",
       " '2020',\n",
       " '-',\n",
       " 'PRESENT',\n",
       " '\\\\n\\\\nCompany',\n",
       " 'Name',\n",
       " ',',\n",
       " 'Location\\\\n\\\\nWhether',\n",
       " 'this',\n",
       " 'position',\n",
       " 'be',\n",
       " 'an',\n",
       " 'internship',\n",
       " ',',\n",
       " 'part-time',\n",
       " ',',\n",
       " 'or',\n",
       " 'full-time',\n",
       " 'position',\n",
       " ',',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'treat',\n",
       " 'the',\n",
       " 'same',\n",
       " '.',\n",
       " 'This',\n",
       " 'be',\n",
       " 'where',\n",
       " 'you',\n",
       " 'write',\n",
       " 'your',\n",
       " 'responsibilities',\n",
       " 'in',\n",
       " 'this',\n",
       " 'position',\n",
       " 'and',\n",
       " 'how',\n",
       " 'your',\n",
       " 'action',\n",
       " 'in',\n",
       " 'your',\n",
       " 'role',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'overall',\n",
       " 'success',\n",
       " 'for',\n",
       " 'the',\n",
       " 'company.\\\\n\\\\n\\\\n\\\\nFREELANCE',\n",
       " 'WORK\\\\n\\\\nWhether',\n",
       " 'this',\n",
       " 'position',\n",
       " 'be',\n",
       " 'an',\n",
       " 'internship',\n",
       " ',',\n",
       " 'part-time',\n",
       " ',',\n",
       " 'or',\n",
       " 'full-time',\n",
       " 'position',\n",
       " ',',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'treat',\n",
       " 'the',\n",
       " 'same',\n",
       " '.',\n",
       " 'This',\n",
       " 'be',\n",
       " 'where',\n",
       " 'you',\n",
       " 'write',\n",
       " 'your',\n",
       " 'responsibilities',\n",
       " 'in',\n",
       " 'this',\n",
       " 'position',\n",
       " 'and',\n",
       " 'how',\n",
       " 'your',\n",
       " 'action',\n",
       " 'in',\n",
       " 'your',\n",
       " 'role',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'overall',\n",
       " 'success',\n",
       " 'for',\n",
       " 'the',\n",
       " 'company.\\\\n\\\\n\\\\nInclude',\n",
       " 'the',\n",
       " 'base-line',\n",
       " 'detail',\n",
       " 'of',\n",
       " 'your',\n",
       " 'freelance',\n",
       " 'work',\n",
       " 'here',\n",
       " '.',\n",
       " 'Include',\n",
       " 'the',\n",
       " 'industry',\n",
       " 'you',\n",
       " 'work',\n",
       " 'in',\n",
       " ',',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'your',\n",
       " 'clients',\n",
       " ',',\n",
       " 'and',\n",
       " 'prove',\n",
       " 'success',\n",
       " 'you',\n",
       " 'generate',\n",
       " 'for',\n",
       " 'them',\n",
       " '.',\n",
       " \"'\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer1 = WordNetLemmatizer()\n",
    "\n",
    "def lemtizer(text):\n",
    "    word_tokens=word_tokenize(text)\n",
    "    \n",
    "    text = [lemmatizer1.lemmatize(word,pos= 'v') for word in word_tokens]\n",
    "    return text\n",
    "\n",
    "lemtizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hex codes\n",
    "text = re.sub(r'[^\\x00-\\x7f]',r'', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2022, 10, 1, 0, 0, tzinfo=<UTC>), datetime.datetime(2121, 10, 10, 0, 0, tzinfo=<UTC>)]\n"
     ]
    }
   ],
   "source": [
    "#Extracting data and time\n",
    "\n",
    "from date_extractor import extract_dates\n",
    "\n",
    "extracted_dates = {}\n",
    "\n",
    "dates = extract_dates(text)\n",
    "print(dates)\n",
    "extracted_dates['DATE'] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Om Surve']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "person_list = []\n",
    "person_names=person_list\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "        \n",
    "        \n",
    "names = get_human_names(text)\n",
    "for person in person_list:\n",
    "    person_split = person.split(\" \")\n",
    "    for name in person_split:\n",
    "        if wordnet.synsets(name):\n",
    "            if(name in person):\n",
    "                person_names.remove(person)\n",
    "                break\n",
    "\n",
    "person_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mordecai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_70532/21242484.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#find places\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmordecai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeoparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgeo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeoparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mordecai'"
     ]
    }
   ],
   "source": [
    "#find places\n",
    "\n",
    "from mordecai import Geoparser\n",
    "geo = Geoparser()\n",
    "geo.geoparse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"    Lorem ipsum Sukhakarta,sector 12,road no 4,New Panvel 410206 Dorem sit amet, 114801 Western East Avenue Apt. B32, Funky Township CA 12345\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114801 Western East Avenue Apt. B32, Funky Township CA 12345]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#finding addresess\n",
    "\n",
    "#Method 1\n",
    "import pyap\n",
    "addresses1 = pyap.parse(text, country='US')\n",
    "print(addresses1)\n",
    "\n",
    "#Method 2\n",
    "import re\n",
    "regexp = \"(?i)\\d+ ((?! \\d+ ).)*(missouri|il|iowa)(, \\d{5}| \\d{5}|\\b)\"\n",
    "address2 = re.match(regexp, text)\n",
    "print(address2)\n",
    "\n",
    "#Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get gender\n",
    "\n",
    "def get_gender(text):\n",
    "    text = nlp(text)\n",
    "    tokens = [token.text.lower().strip() for token in text]   \n",
    "\n",
    "    if 'female' in tokens:\n",
    "        gender = 'Female'\n",
    "    elif 'male' in tokens:\n",
    "        gender = 'Male'\n",
    "    else:\n",
    "        gender = 'not mentioned'\n",
    "    return gender\n",
    "\n",
    "def get_pincode(text):\n",
    "    pincode =  r\"[^\\d][^a-zA-Z\\d](\\d{6})[^a-zA-Z\\d]\"\n",
    "    pattern = re.compile(pincode)\n",
    "    result = pattern.findall(text)\n",
    "    if len(result)==0:\n",
    "        return ' '\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "def get_dob(text, ents):\n",
    "    \n",
    "\n",
    "        dob = 'Not found'\n",
    "        lines = [line.strip() for line in text.split('\\n')]\n",
    "        dob_pattern = r'((\\d)?(\\d)(th)?.((jan)|(feb)|(mar)|(apr)|(may)|(jun)|(jul)|(aug)|(sep)|(oct)|(nov)|(dec)|(january)|(february)|(march)|(april)|(may)|(june)|(july)|(august)|(september)|(october)|(november)|(december)|(\\d{2})).(\\d{4}))'\n",
    "        required = ''\n",
    "        matches = ['dob', 'date of birth', 'birth date']\n",
    "        flag = 0\n",
    "        count = 0\n",
    "        for lin in lines:\n",
    "            \n",
    "            if any(x in lin.lower().strip() for x in matches):\n",
    "                required = lin.lower() +'\\n'\n",
    "                flag = 1\n",
    "            if flag == 1:\n",
    "                if len(lin.split()) < 1:continue\n",
    "                required += lin.lower() + '\\n'\n",
    "                count +=1\n",
    "            if count > 4:\n",
    "                break        \n",
    "        required = ' '.join(req for req in required.split())\n",
    "\n",
    "        match = re.findall(dob_pattern, required)\n",
    "        try:\n",
    "            return match[0][0]\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "# Function for obtaining language\n",
    "def get_lang(text):\n",
    "    text = nlp(text)\n",
    "    language = text._.language[\"language\"]\n",
    "    if language == 'en':\n",
    "        language = 'English'\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def captch_ex(file_name):\n",
    "    img = cv2.imread(file_name)\n",
    "\n",
    "    img_final = cv2.imread(file_name)\n",
    "    img2gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, mask = cv2.threshold(img2gray, 180, 255, cv2.THRESH_BINARY)\n",
    "    image_final = cv2.bitwise_and(img2gray, img2gray, mask=mask)\n",
    "    ret, new_img = cv2.threshold(image_final, 180, 255, cv2.THRESH_BINARY)  # for black text , cv.THRESH_BINARY_INV\n",
    "    '''\n",
    "            line  8 to 12  : Remove noisy portion \n",
    "    '''\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3,\n",
    "                                                         3))  # to manipulate the orientation of dilution , large x means horizonatally dilating  more, large y means vertically dilating more\n",
    "    dilated = cv2.dilate(new_img, kernel, iterations=9)  # dilate , more the iteration more the dilation\n",
    "\n",
    "    # for cv2.x.x\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)  # findContours returns 3 variables for getting contours\n",
    "\n",
    "    # for cv3.x.x comment above line and uncomment line below\n",
    "\n",
    "    #image, contours, hierarchy = cv2.findContours(dilated,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\n",
    "    for contour in contours:\n",
    "        # get rectangle bounding contour\n",
    "        [x, y, w, h] = cv2.boundingRect(contour)\n",
    "\n",
    "        # Don't plot small false positives that aren't text\n",
    "        if w < 35 and h < 35:\n",
    "            continue\n",
    "\n",
    "        # draw rectangle around contour on original image\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 255), 2)\n",
    "\n",
    "        '''\n",
    "        #you can crop image and send to OCR  , false detected will return no text :)\n",
    "        cropped = img_final[y :y +  h , x : x + w]\n",
    "\n",
    "        s = file_name + '/crop_' + str(index) + '.jpg' \n",
    "        cv2.imwrite(s , cropped)\n",
    "        index = index + 1\n",
    "\n",
    "        '''\n",
    "    # write original image with added contours to disk\n",
    "    cv2.imshow('captcha_result', img)\n",
    "    cv2.waitKey()\n",
    "\n",
    "\n",
    "i1 = \"/Users/cosmos/Documents/1r.jpeg\"\n",
    "i2 = '/Users/cosmos/Documents/2r.jpeg'\n",
    "i3 = '/Users/cosmos/Documents/3r.jpeg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "captch_ex(i3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 53, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 24, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 9, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 10 , sMin = 9, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 42 , sMin = 9, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 42 , sMin = 7, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 42 , sMin = 7, vMin = 0), (hMax = 57 , sMax = 255, vMax = 255)\n",
      "(hMin = 42 , sMin = 7, vMin = 0), (hMax = 40 , sMax = 255, vMax = 255)\n",
      "(hMin = 42 , sMin = 7, vMin = 0), (hMax = 19 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 7, vMin = 0), (hMax = 19 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 19 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 57 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 135 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 156 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 220, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 151, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 120, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 94, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 0, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 45), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 138), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 194), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 255), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 235), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 127), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 61 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 133 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 133 , sMax = 187, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 187, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 118, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 27, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 10 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 3 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 0 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 12, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 176 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 160 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 235, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 209, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 153, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 0, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 19, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 227)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 186)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 82)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 7), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 27), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 52), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 120), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 157), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 184), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 206), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 220), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 255), (hMax = 179 , sMax = 255, vMax = 255)\n",
      "(hMin = 2 , sMin = 0, vMin = 0), (hMax = 179 , sMax = 255, vMax = 255)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2h/lw5rv90j0ln7v084d1vkrcg80000gn/T/ipykernel_71439/524860603.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Display result image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread('/Users/cosmos/Documents/1r.jpeg')\n",
    "\n",
    "# Create a window\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "# Create trackbars for color change\n",
    "# Hue is from 0-179 for Opencv\n",
    "cv2.createTrackbar('HMin', 'image', 0, 179, nothing)\n",
    "cv2.createTrackbar('SMin', 'image', 0, 255, nothing)\n",
    "cv2.createTrackbar('VMin', 'image', 0, 255, nothing)\n",
    "cv2.createTrackbar('HMax', 'image', 0, 179, nothing)\n",
    "cv2.createTrackbar('SMax', 'image', 0, 255, nothing)\n",
    "cv2.createTrackbar('VMax', 'image', 0, 255, nothing)\n",
    "\n",
    "# Set default value for Max HSV trackbars\n",
    "cv2.setTrackbarPos('HMax', 'image', 179)\n",
    "cv2.setTrackbarPos('SMax', 'image', 255)\n",
    "cv2.setTrackbarPos('VMax', 'image', 255)\n",
    "\n",
    "# Initialize HSV min/max values\n",
    "hMin = sMin = vMin = hMax = sMax = vMax = 0\n",
    "phMin = psMin = pvMin = phMax = psMax = pvMax = 0\n",
    "\n",
    "while(1):\n",
    "    # Get current positions of all trackbars\n",
    "    hMin = cv2.getTrackbarPos('HMin', 'image')\n",
    "    sMin = cv2.getTrackbarPos('SMin', 'image')\n",
    "    vMin = cv2.getTrackbarPos('VMin', 'image')\n",
    "    hMax = cv2.getTrackbarPos('HMax', 'image')\n",
    "    sMax = cv2.getTrackbarPos('SMax', 'image')\n",
    "    vMax = cv2.getTrackbarPos('VMax', 'image')\n",
    "\n",
    "    # Set minimum and maximum HSV values to display\n",
    "    lower = np.array([hMin, sMin, vMin])\n",
    "    upper = np.array([hMax, sMax, vMax])\n",
    "\n",
    "    # Convert to HSV format and color threshold\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask = cv2.inRange(hsv, lower, upper)\n",
    "    result = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    # Print if there is a change in HSV value\n",
    "    if((phMin != hMin) | (psMin != sMin) | (pvMin != vMin) | (phMax != hMax) | (psMax != sMax) | (pvMax != vMax) ):\n",
    "        print(\"(hMin = %d , sMin = %d, vMin = %d), (hMax = %d , sMax = %d, vMax = %d)\" % (hMin , sMin , vMin, hMax, sMax , vMax))\n",
    "        phMin = hMin\n",
    "        psMin = sMin\n",
    "        pvMin = vMin\n",
    "        phMax = hMax\n",
    "        psMax = sMax\n",
    "        pvMax = vMax\n",
    "\n",
    "    # Display result image\n",
    "    cv2.imshow('image', result)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "\n",
    "#pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Load image, convert to HSV format, define lower/upper ranges, and perform\n",
    "# color segmentation to create a binary mask\n",
    "image = cv2.imread('/Users/cosmos/Documents/2r.jpeg')\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "lower = np.array([0, 0, 218])\n",
    "upper = np.array([157, 54, 255])\n",
    "mask = cv2.inRange(hsv, lower, upper)\n",
    "\n",
    "# Create horizontal kernel and dilate to connect text characters\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,3))\n",
    "dilate = cv2.dilate(mask, kernel, iterations=5)\n",
    "\n",
    "# Find contours and filter using aspect ratio\n",
    "# Remove non-text contours by filling in the contour\n",
    "cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "for c in cnts:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    ar = w / float(h)\n",
    "    if ar < 5:\n",
    "        cv2.drawContours(dilate, [c], -1, (0,0,0), -1)\n",
    "\n",
    "# Bitwise dilated image with mask, invert, then OCR\n",
    "result = 255 - cv2.bitwise_and(dilate, mask)\n",
    "data = pytesseract.image_to_string(result, lang='eng',config='--psm 6')\n",
    "print(data)\n",
    "\n",
    "cv2.imshow('mask', mask)\n",
    "cv2.imshow('dilate', dilate)\n",
    "cv2.imshow('result', result)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
