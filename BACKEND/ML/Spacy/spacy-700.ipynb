{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "import random\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "#train data\n",
    "train_data= pickle.load(open('/Users/cosmos/Desktop/DeepBlue/BACKEND/ML/Datasets (UNCLEANED)/train_data_700.pkl','rb'))\n",
    "#train_data= pickle.load(open('/content/drive/MyDrive/Colab Notebooks/datasets/d.pkl','rb'))\n",
    "\n",
    "#defining the traning model\n",
    "\n",
    "def train_model(train_data):\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner=nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner,last=True)\n",
    "        \n",
    "    for _, annotation in train_data:\n",
    "        for ent in annotation['entities']:\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "# Now to remove other pipelines \n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "            optimizer = nlp.begin_training()\n",
    "            for itn in range(50):\n",
    "                print(\"Statring iteration \" + str(itn))\n",
    "                random.shuffle(train_data)\n",
    "                losses = {}\n",
    "                index = 0\n",
    "                for text, annotations in train_data:\n",
    "                    try:\n",
    "                        nlp.update(\n",
    "                            [text],  # batch of texts\n",
    "                            [annotations],  # batch of annotations\n",
    "                            drop=0.2,  # dropout - make it harder to memorise data\n",
    "                            sgd=optimizer,  # callable to update weights\n",
    "                            losses=losses)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                    \n",
    "                print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_data) #7159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('/content/drive/MyDrive/Colab Notebooks/Models/DeepBlue/700-new-code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import textract\n",
    "import re\n",
    "from tika import parser \n",
    "fname = '/content/drive/MyDrive/Colab Notebooks/Models/DeepBlue/Test- Resumes/Resumes/Resume 2 pdf/type_14.pdf'\n",
    "\n",
    "# text1 = textract.process('/content/drive/MyDrive/Colab Notebooks/Models/DeepBlue/Test- Resumes/Profile.pdf')\n",
    "# text2 = textract.process('/content/drive/MyDrive/Colab Notebooks/Models/DeepBlue/Test- Resumes/test.pdf')\n",
    "\n",
    "raw = parser.from_file(fname)\n",
    "print(raw['content'])\n",
    "\n",
    "\n",
    "def pre_process(text):\n",
    "  text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "  text = re.sub(\"\\n\", \" \",text)\n",
    "  return text\n",
    "\n",
    "#pre_process(text2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_data= pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Models/DeepBlue/spacy/train_data.pkl','rb')) #trying model 2 bigger dataset\n",
    "\n",
    "\n",
    "doc = nlp_model(raw['content'])\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.label_.upper():{30}} - {ent.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
