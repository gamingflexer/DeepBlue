{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__int__()\n",
    "        self.bert = BertForTokenClassification.from_pretrained(\n",
    "            '/content/drive/MyDrive/Colab Notebooks/DeepBlue/bert-large-uncased', num_labels=len(tag2idx))\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There are two ways to do it: Since you are looking to fine-tune the model for a downstream task similar to classification, you can directly use:\n",
    "\n",
    "BertForSequenceClassification class. Performs fine-tuning of logistic regression layer on the output dimension of 768.\n",
    "\n",
    "Alternatively, you can define a custom module, that created a bert model based on the pre-trained weights and adds layers on top of it.\n",
    "\n",
    "from transformers import BertModel\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.bert = BertModel.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")\n",
    "          ### New layers:\n",
    "          self.linear1 = nn.Linear(768, 256)\n",
    "          self.linear2 = nn.Linear(256, 3) ## 3 is the number of classes in this example\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "          sequence_output, pooled_output = self.bert(\n",
    "               ids, \n",
    "               attention_mask=mask)\n",
    "\n",
    "          # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "          linear1_output = self.linear1(sequence_output[:,0,:].view(-1,768)) ## extract the 1st token's embeddings\n",
    "\n",
    "          linear2_output = self.linear2(linear2_output)\n",
    "\n",
    "          return linear2_output\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")\n",
    "model = CustomBERTModel() # You can pass the parameters if required to have more flexible model\n",
    "model.to(torch.device(\"cpu\")) ## can be gpu\n",
    "criterion = nn.CrossEntropyLoss() ## If required define your own criterion\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "for epoch in epochs:\n",
    "    for batch in data_loader: ## If you have a DataLoader()  object to get the data.\n",
    "\n",
    "        data = batch[0]\n",
    "        targets = batch[1] ## assuming that data loader returns a tuple of data and its targets\n",
    "        \n",
    "        optimizer.zero_grad()   \n",
    "        encoding = tokenizer.batch_encode_plus(data, return_tensors='pt', padding=True, truncation=True,max_length=50, add_special_tokens = True)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        outputs = F.log_softmax(outputs, dim=1)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Combination of glyph and BiLSTM-CRF model\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import stack_bidirectional_dynamic_rnn as BiRNN\n",
    "from ner.optim import get_training_op\n",
    "\n",
    "\n",
    "def _get_transition_params(num_tags):\n",
    "    with tf.variable_scope(\"crf\", reuse=tf.AUTO_REUSE):\n",
    "        transition_params = tf.get_variable(\n",
    "            \"transition_params\",\n",
    "            [num_tags, num_tags],\n",
    "            trainable=True,\n",
    "            initializer=tf.random_uniform_initializer)\n",
    "    return transition_params\n",
    "\n",
    "\n",
    "class BiLSTMCRFModel():\n",
    "    \"\"\"\n",
    "    BiLSTM Model that gets embedding from BERT and GLYPH-CNN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "\n",
    "        self.hp = hparams\n",
    "        self.scaffold = None\n",
    "        self.transition_params = None\n",
    "\n",
    "    def embed(self, *, inputs, is_training):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def body(self, *, inputs, mode):\n",
    "        \"\"\" Return token-level logits \"\"\"\n",
    "\n",
    "        is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "        # Get token embeddings\n",
    "        token_embeddings = self.embed(inputs=inputs, is_training=is_training)\n",
    "\n",
    "        # Build up sentence encoder\n",
    "        rnn_keep_prob = 0.5 if is_training else 1.\n",
    "\n",
    "        def build_rnn_cell():\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(256)\n",
    "\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                cell,\n",
    "                output_keep_prob=rnn_keep_prob,\n",
    "                input_keep_prob=rnn_keep_prob)\n",
    "            return cell\n",
    "\n",
    "        fw_cells = [build_rnn_cell()]\n",
    "        bw_cells = [build_rnn_cell()]\n",
    "\n",
    "        # encode inputs using RNN encoder\n",
    "        seq_length = tf.reshape(\n",
    "            inputs[\"INPUT_SEQUENCE_LENGTH\"], [-1])\n",
    "        outputs, _, _ = BiRNN(fw_cells, bw_cells, token_embeddings,\n",
    "                              sequence_length=seq_length, dtype=tf.float32)\n",
    "\n",
    "        # convert encoded inputs to logits\n",
    "        logits = tf.layers.dense(\n",
    "            outputs,\n",
    "            self.hp.output_dim,\n",
    "            use_bias=True)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def loss(self, *, predictions, features, targets, is_training):\n",
    "        \"\"\" For a CRF, predictions should be token-level logits and\n",
    "        targets should be indexed labels.\n",
    "        \"\"\"\n",
    "        del is_training\n",
    "        seq_lens = tf.reshape(\n",
    "            features[\"INPUT_SEQUENCE_LENGTH\"], [-1])\n",
    "        transition_params = _get_transition_params(self.hp.output_dim)\n",
    "        with tf.control_dependencies(\n",
    "                [tf.compat.v1.assert_less(targets, tf.cast(self.hp.output_dim, tf.int64))]):\n",
    "\n",
    "            likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n",
    "                predictions,\n",
    "                targets,\n",
    "                seq_lens,\n",
    "                transition_params=transition_params)\n",
    "\n",
    "        return tf.reduce_mean(-likelihood)\n",
    "\n",
    "\n",
    "    def predict_from_logits(self, *, logits, features):\n",
    "        \"\"\" Do CRF decoding starting from logits, rather than raw input \"\"\"\n",
    "        seq_lens = tf.reshape(\n",
    "            features[\"INPUT_SEQUENCE_LENGTH\"], [-1])\n",
    "        transition_params = _get_transition_params(self.hp.output_dim)\n",
    "        predictions, _ = tf.contrib.crf.crf_decode(\n",
    "            logits,\n",
    "            transition_params,\n",
    "            seq_lens)\n",
    "\n",
    "        return {\n",
    "            \"PREDICTED_TAGS\": predictions,\n",
    "        }\n",
    "\n",
    "    def get_model_fn(self, model_dir=None):\n",
    "\n",
    "\n",
    "        def fn(features, labels, mode, params):\n",
    "            del params\n",
    "            is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "            logits = self.body(inputs=features, mode=mode)\n",
    "\n",
    "            if is_training:\n",
    "                loss = self.loss(predictions=logits,\n",
    "                                 features=features, targets=labels,\n",
    "                                 is_training=is_training)\n",
    "                train_op = get_training_op(loss, self.hp)\n",
    "                return tf.estimator.EstimatorSpec(mode, loss=loss,\n",
    "                                                  train_op=train_op,\n",
    "                                                  scaffold=self.scaffold)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "                predictions = self.predict_from_logits(\n",
    "                    logits=logits, features=features)\n",
    "                predictions[\"PREDICTION_LENGTH\"] = tf.reshape(\n",
    "                    features[\"INPUT_SEQUENCE_LENGTH\"], [-1]\n",
    "                )\n",
    "                est = tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "                return est\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.EVAL:\n",
    "                loss = self.loss(predictions=logits, features=features,\n",
    "                                 targets=labels, is_training=is_training)\n",
    "\n",
    "                predictions = self.predict_from_logits(\n",
    "                    logits=logits, features=features)\n",
    "\n",
    "                seq_lens = tf.reshape(features[\"INPUT_SEQUENCE_LENGTH\"], [-1])\n",
    "                weights = tf.sequence_mask(seq_lens, dtype=tf.float32)\n",
    "\n",
    "                predicted_labels = predictions[\"PREDICTED_TAGS\"]\n",
    "\n",
    "                eval_metrics = {\n",
    "                    'accuracy': tf.metrics.accuracy(labels, predicted_labels, weights=weights)\n",
    "                }\n",
    "\n",
    "                return tf.estimator.EstimatorSpec(\n",
    "                    mode,\n",
    "                    loss=loss,\n",
    "                    eval_metric_ops=eval_metrics)\n",
    "\n",
    "        return fn\n",
    "\n",
    "\n",
    "class GlyphCRF(BiLSTMCRFModel):\n",
    "    def __init__(self, hparams):\n",
    "        BiLSTMCRFModel.__init__(self, hparams)\n",
    "\n",
    "        self.bert = BERT(hparams)\n",
    "        self.glyph_embedding_initializer = None\n",
    "\n",
    "    def embed(self, *, inputs, is_training):\n",
    "        features = []\n",
    "        raw_images = inputs[\"GLYPH_FEATURE_SEQUENCE\"]\n",
    "        batch_size = tf.shape(raw_images)[0]\n",
    "        batch_len = tf.shape(raw_images)[1]\n",
    "        reshaped_images = tf.reshape(raw_images, [batch_size * batch_len, 64, 64, 1])\n",
    "        reshaped_images /= 255.0\n",
    "        if self.hp.glyph_encoder == 'strided':\n",
    "            encoder = strided_glyph_encoder(self.hp)\n",
    "        elif self.hp.glyph_encoder == 'glyce_cnn':\n",
    "            encoder = cnn_glyph_encoder(self.hp)\n",
    "        else:\n",
    "            raise ValueError(self.hparams.glyph_encoder)\n",
    "        codes = encoder(reshaped_images, is_training)\n",
    "        output_dim = codes.get_shape().as_list()[-1]\n",
    "        reshaped_codes = tf.reshape(codes, [batch_size, batch_len, output_dim])\n",
    "        features.append(reshaped_codes)\n",
    "\n",
    "        features += [self.bert.embed(inputs=inputs,\n",
    "                                        is_training=False)]\n",
    "\n",
    "        if len(features) > 1:\n",
    "            return tf.concat(features, axis=-1)\n",
    "        else:\n",
    "            return features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "\n",
    "    model.train(True)\n",
    "    for x_batch, y_batch, batch in generate_batch_data(x_train, y_train, batch_size):\n",
    "        y_pred = model(x_batch)\n",
    "        y_batch = y_batch.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= batch\n",
    "    train_losses.append(train_loss)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    model.eval() # disable dropout for deterministic output\n",
    "    with torch.no_grad(): # deactivate autograd engine to reduce memory usage and speed up computations\n",
    "        val_loss, batch = 0, 1\n",
    "        for x_batch, y_batch, batch in generate_batch_data(x_val, y_val, batch_size):\n",
    "            y_pred = model(x_batch)\n",
    "            y_batch = y_batch.unsqueeze(1)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= batch\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    print(\n",
    "        \"Epoch %d Train loss: %.2f. Validation loss: %.2f. Elapsed time: %.2fs.\"\n",
    "        % (epoch + 1, train_losses[-1], val_losses[-1], elapsed)\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
